2024-07-03 09:52:39.872274: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-03 09:52:39.918100: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-03 09:52:40.521623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
begin to verify or download CIFAR10 dataset
Tisfy: Files already downloaded and verified.
Tisfy: Files already downloaded and verified.
verify or download CIFAR10 dataset successfully
Some weights of ViTForImageClassification were not initialized from the model checkpoint at data/vit_base_patch16_224/pytorch_model.bin and are newly initialized because the shapes did not match:
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO :      
INFO :      Received: get_parameters message 563e18b9-113b-4007-b817-84a5dbe953cf
INFO :      Sent reply
INFO :      
INFO :      Received: train message de09f88d-0cf9-4acd-b3d7-b0a1d3494ad9
ERROR :     Client raised an exception.
Traceback (most recent call last):
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/app.py", line 379, in _start_client_internal
    reply_message = client_app(message=message, context=context)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/lzy/ltf/Codes/FLDefinder/master/src/client.py", line 31, in fit
    self.train()
  File "/home/lzy/ltf/Codes/FLDefinder/master/src/client.py", line 47, in train
    outputs = self.model(images).logits
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 831, in forward
    outputs = self.vit(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 614, in forward
    encoder_outputs = self.encoder(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 443, in forward
    layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 388, in forward
    self_attention_outputs = self.attention(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 318, in forward
    self_outputs = self.attention(hidden_states, head_mask, output_attentions)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 212, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.70 GiB total capacity; 5.41 GiB already allocated; 4.81 MiB free; 5.54 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "main_client.py", line 6, in <module>
    client.main()
  File "/home/lzy/ltf/Codes/FLDefinder/master/src/client.py", line 81, in main
    fl.client.start_client(server_address="localhost:8080", client=client.to_client())
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/app.py", line 157, in start_client
    _start_client_internal(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/app.py", line 386, in _start_client_internal
    raise ex
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/app.py", line 379, in _start_client_internal
    reply_message = client_app(message=message, context=context)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/lzy/ltf/Codes/FLDefinder/master/src/client.py", line 31, in fit
    self.train()
  File "/home/lzy/ltf/Codes/FLDefinder/master/src/client.py", line 47, in train
    outputs = self.model(images).logits
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 831, in forward
    outputs = self.vit(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 614, in forward
    encoder_outputs = self.encoder(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 443, in forward
    layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 388, in forward
    self_attention_outputs = self.attention(
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 318, in forward
    self_outputs = self.attention(hidden_states, head_mask, output_attentions)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py", line 212, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.70 GiB total capacity; 5.41 GiB already allocated; 4.81 MiB free; 5.54 GiB reserved in total by PyTorch)