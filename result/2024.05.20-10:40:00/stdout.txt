NVIDIA GeForce RTX 3090
Running Self-Attention Gradient Attack on a blend of ViT-L-16 and BiT-M-R101x3
Files already downloaded and verified
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
/home/lzy/.conda/envs/ltf/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
Clean Acc ViT-L_16: 1.0
Clean Acc BiT-M-R101x3: 1.0
ViT-L_16 robust Acc: 1.0
BiT-M-R101x3 robust Acc: 0.004
blend robust acc:  0.511
Warning, model ViT-L_16 is being deleted and should not be called again!
Warning, model BiT-M-R101x3 is being deleted and should not be called again!
Elapsed time:  1217.0135986804962