@inproceedings{betterTogether,
    title     = {Better together: Attaining the triad of Byzantine-robust federated learning via local update amplification},
    author    = {Shen, Liyue and Zhang, Yanjun and Wang, Jingwei and Bai, Guangdong},
    booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
    pages     = {201--213},
    year      = {2022}
}

@article{BetterTogether24,
    title    = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
    journal  = {arXiv: Learning,arXiv: Learning},
    author   = {McMahan, H.Brendan and Moore, EiderB and Ramage, Daniel and Hampson, Seth and Arcas, BlaiseAgüeray},
    year     = {2016},
    month    = {Feb},
    language = {en-US}
}

@inproceedings{BetterTogether29,
    title     = {Privacy-preserving deep learning},
    url       = {http://dx.doi.org/10.1109/allerton.2015.7447103},
    doi       = {10.1109/allerton.2015.7447103},
    booktitle = {2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
    author    = {Shokri, Reza and Shmatikov, Vitaly},
    year      = {2015},
    month     = {Sep},
    language  = {en-US}
}

@comment{FeSTA用于COVID-19胸部X光片诊断，通过分割学习（split learning）将神经网络分为客户端和服务器部分，从而减少了带宽消耗。该框架在多个来源的数据集上实现了性能提升，同时保留了数据隐私​ (NeurIPS Proceedings)​。}
@article{federatedViT_example,
    title   = {Federated split task-agnostic vision transformer for COVID-19 CXR diagnosis},
    author  = {Park, Sangjoon and Kim, Gwanghyun and Kim, Jeongsol and Kim, Boah and Ye, Jong Chul},
    journal = {Advances in Neural Information Processing Systems},
    volume  = {34},
    pages   = {24617--24630},
    year    = {2021}
}

% 深度神经网络的潜在后门攻击
% CCF-A
@inproceedings{latentAttack,
    title     = {Latent backdoor attacks on deep neural networks},
    author    = {Yao, Yuanshun and Li, Huiying and Zheng, Haitao and Zhao, Ben Y},
    booktitle = {Proceedings of the 2019 ACM SIGSAC conference on computer and communications security},
    pages     = {2041--2055},
    year      = {2019}
}

% 尾部攻击：是的，你确实可以对联邦学习进行后门攻击
% CCF-A
% 有涉及标签翻转
@article{tailAttack_SuchAsLabelFlip,
    title   = {Attack of the tails: Yes, you really can backdoor federated learning},
    author  = {Wang, Hongyi and Sreenivasan, Kartik and Rajput, Shashank and Vishwakarma, Harit and Agarwal, Saurabh and Sohn, Jy-yong and Lee, Kangwook and Papailiopoulos, Dimitris},
    journal = {Advances in Neural Information Processing Systems},
    volume  = {33},
    pages   = {16070--16084},
    year    = {2020}
}

% 后门攻击
% CCF-A
@inproceedings{backdoor_001,
    title     = {Invisible backdoor attack with sample-specific triggers},
    author    = {Li, Yuezun and Li, Yiming and Wu, Baoyuan and Li, Longkang and He, Ran and Lyu, Siwei},
    booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
    pages     = {16463--16472},
    year      = {2021}
}

% 通过在服务器上计算每个模型更新与其最近更新之间的欧氏距离之和来选择聚合模型的Krum算法
% CCF-A
@article{aggregation_Krum,
    title   = {Machine learning with adversaries: Byzantine tolerant gradient descent},
    author  = {Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
    journal = {Advances in neural information processing systems},
    volume  = {30},
    year    = {2017}
}

% 选择中值或排除边缘值后的平均值作为全局模型的中值算法和裁剪平均算法
% CCF-A
@inproceedings{aggregation_MedianTrimmedMean,
    title        = {Byzantine-robust distributed learning: Towards optimal statistical rates},
    author       = {Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
    booktitle    = {International conference on machine learning},
    pages        = {5650--5659},
    year         = {2018},
    organization = {Pmlr}
}

% Fed-PCA
% CCF-None
@inproceedings{Fed_PCA,
    title        = {Data-free evaluation of user contributions in federated learning},
    author       = {Lv, Hongtao and Zheng, Zhenzhe and Luo, Tie and Wu, Fan and Tang, Shaojie and Hua, Lifeng and Jia, Rongfei and Lv, Chengfei},
    booktitle    = {2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt)},
    pages        = {1--8},
    year         = {2021},
    organization = {IEEE}
}

% 通过解决最大团问题(Maximum CliqueProblem, MCP)从而无需无需恶意用户数量这一先验知识的Sniper方案
% CCF-C
@inproceedings{aggregation_Sniper,
    title        = {Understanding distributed poisoning attack in federated learning},
    author       = {Cao, Di and Chang, Shan and Lin, Zhijian and Liu, Guohua and Sun, Donghong},
    booktitle    = {2019 IEEE 25th international conference on parallel and distributed systems (ICPADS)},
    pages        = {233--239},
    year         = {2019},
    organization = {IEEE}
}

% 主观逻辑模型
@article{Subjective_Logic_Model,
    title    = {Incentive Mechanism for Reliable Federated Learning: A Joint Optimization Approach to Combining Reputation and Contract Theory},
    url      = {http://dx.doi.org/10.1109/jiot.2019.2940820},
    doi      = {10.1109/jiot.2019.2940820},
    journal  = {IEEE Internet of Things Journal},
    author   = {Kang, Jiawen and Xiong, Zehui and Niyato, Dusit and Xie, Shengli and Zhang, Junshan},
    year     = {2019},
    month    = {Dec},
    pages    = {10700–10714},
    language = {en-US}
}

% 这篇论文主要讨论了机器学习和边缘计算的结合应用，涵盖了其带来的机会、面临的挑战、现有平台和框架以及实际使用案例。文章重点分析了边缘计算如何通过减少数据传输量、提高可扩展性和降低延迟来应对数据量的快速增长​ (MDPI)​
% CCF-None
@article{edgeComputing_explosiveGrowth,
    title     = {Combining Machine Learning and Edge Computing: Opportunities, Challenges, Platforms, Frameworks, and Use Cases},
    author    = {Grzesik, Piotr and Mrozek, Dariusz},
    journal   = {Electronics},
    volume    = {13},
    number    = {3},
    pages     = {640},
    year      = {2024},
    publisher = {MDPI}
}

% 2024年，全球大约75%的人口将受到现代隐私法规的保护。这些法规的扩展推动了隐私实践的实施，并且企业在数据隐私方面的年均预算预计将超过250万美元
% 网络文章
@misc{dataPrivacyIsEncreasing,
    author = {Nader Henein},
    title  = {Gartner Identifies Top Five Trends in Privacy Through 2024},
    year   = 2022,
    url    = {https://www.gartner.com/en/newsroom/press-releases/2022-05-31-gartner-identifies-top-five-trends-in-privacy-through-2024},
    note   = {Accessed: 2024-07-08}
}

% 为了解决数据量激增和隐私包含的问题，一种越来越流行的方式是使用联邦学习
% 《Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments》的intro里引用了这篇
% CCF-None，但被引用了5k多次
@article{useFL2solve,
    title     = {Advances and open problems in federated learning},
    author    = {Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
    journal   = {Foundations and trends{\textregistered} in machine learning},
    volume    = {14},
    number    = {1--2},
    pages     = {1--210},
    year      = {2021},
    publisher = {Now Publishers, Inc.}
}

% 联邦学习创世纪paper
% CCF-C 16493被引
@inproceedings{FLGenesisArticle,
    title        = {Communication-efficient learning of deep networks from decentralized data},
    author       = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
    booktitle    = {Artificial intelligence and statistics},
    pages        = {1273--1282},
    year         = {2017},
    organization = {PMLR}
}

% Transformer
% CCF-A
@article{transformer,
    title   = {Attention is all you need},
    author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal = {Advances in neural information processing systems},
    volume  = {30},
    year    = {2017}
}

% Transformer可以运用在翻译领域
% CCF-None
@article{transformer_translation,
    title   = {Lessons on parameter sharing across layers in transformers},
    author  = {Takase, Sho and Kiyono, Shun},
    journal = {arXiv preprint arXiv:2104.06022},
    year    = {2021}
}

% Transformer可以运用在音频分类领域
% CCF-A
@article{transformer_audioClassification,
    title   = {Attention bottlenecks for multimodal fusion},
    author  = {Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
    journal = {Advances in neural information processing systems},
    volume  = {34},
    pages   = {14200--14213},
    year    = {2021}
}

% ViT
% CCF-None，被引38933次
@article{transformer_vision,
    title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
    author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
    journal = {arXiv preprint arXiv:2010.11929},
    year    = {2020}
}

% transformer引起了业界和学术界的兴趣
% CCF-None
@article{transformer_gotInterest,
    title   = {Scaling language model size in cross-device federated learning},
    author  = {Ro, Jae Hun and Breiner, Theresa and McConnaughey, Lara and Chen, Mingqing and Suresh, Ananda Theertha and Kumar, Shankar and Mathews, Rajiv},
    journal = {arXiv preprint arXiv:2204.09715},
    year    = {2022}
}